<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no"> <meta name="author" content="OHBM Open-science Special Interest Group"> <meta name="description" content="OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG."> <meta name="keywords" content="brainhack, neuroscience, AI"> <!-- <meta name="google-site-verification" content="" /> <link rel="canonical" href="https://ohbm.github.io/hackathon2022"> --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:site" content="@OhbmOpen"> <meta name="twitter:title" content="HackTrack"> <meta name="twitter:description" content="OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG."> <meta name="twitter:image:src" content="https://ohbm.github.io/hackathon2022/hackathon2022"> <meta property="og:title" content="HackTrack" /> <meta property="og:site_name" content="OHBM BrainHack 2022" /> <meta property="og:type" content="website" /> <meta property="og:url" content="https://ohbm.github.io/hackathon2022" /> <meta property="og:image" content="https://ohbm.github.io/hackathon2022/hackathon2022" /> <meta property="og:image:width" content="700" /> <meta property="og:image:height" content="350" /> <meta property="og:description" content="OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG." /> <title>HackTrack &bull; OHBM BrainHack 2022</title> <link rel="shortcut icon" href="/hackathon2022/img/favicons/favicon.ico"> <link rel="apple-touch-icon" sizes="152x152" href="/hackathon2022/img/favicons/apple-icon-152x152.png"> <link rel="apple-touch-icon" sizes="144x144" href="/hackathon2022/img/favicons/apple-icon-144x144.png"> <link rel="apple-touch-icon" sizes="120x120" href="/hackathon2022/img/favicons/apple-icon-120x120.png"> <link rel="apple-touch-icon" sizes="114x114" href="/hackathon2022/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="76x76" href="/hackathon2022/img/favicons/apple-icon-76x76.png"> <link rel="apple-touch-icon" sizes="72x72" href="/hackathon2022/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="60x60" href="/hackathon2022/img/favicons/apple-icon-60x60.png"> <link rel="apple-touch-icon" sizes="57x57" href="/hackathon2022/img/favicons/apple-icon-57x57.png"> <link rel="icon" type="image/png" href="/hackathon2022/img/favicons/favicon-96x96.png"> <link rel="icon" type="image/png" href="/hackathon2022/img/favicons/favicon-32x32.png"> <link rel="icon" type="image/png" href="/hackathon2022/img/favicons/favicon-16x16.png"> <meta name="msapplication-TileColor" content="#2b5797"> <meta name="msapplication-TileImage" content="/hackathon2022/img/favicons/mstile-144x144.png"> <meta name="msapplication-config" content="/hackathon2022/img/favicons/browserconfig.xml"> <meta name="theme-color" content="#2b5797"> <link href="/hackathon2022/css/main.css" rel="stylesheet"> <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries --> <!-- WARNING: Respond.js doesn't work if you view the page via file:// --> <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]--> </head> <body> <div id="preloader" class="preloader"> <div class="loader-gplus"></div> </div> <div id="st-container" class="st-container disable-scrolling"> <div class="st-pusher"> <div class="st-content"> <!-- Begin Top Section --> <section id="top-section" class="top-section image-section enable-overlay" style="background-image: url('/hackathon2022/img/sections-background/25900655562_27549efb87_o.jpg');"> <div class="overlay gradient-overlay"></div> <header id="top-header" class="top-header"> <div class="overlay white-solid"></div> <svg id="menu-trigger" class="menu-trigger icon icon-menu visible-xs visible-sm visible-md" viewBox="0 0 32 32"> <use xlink:href="/hackathon2022/img/sprites/sprites.svg#icon-menu"></use> </svg> <a href="/hackathon2022/" id="logo-header" class="logo-header"> <div class="logo logo-light"></div> </a> <nav class="st-menu st-effect" id="menu"> <div class="logo-navbar logo logo-dark visible-xs visible-sm"></div> <ul> <li> <a class="" href=" /hackathon2022/schedule " >Schedule</a> </li> <li> <a class="" href=" /hackathon2022/hacktrack " >HackTrack</a> </li> <li> <a class="" href=" /hackathon2022/traintrack/ " >TrainTrack Corner</a> </li> <li> <a class="" href=" /hackathon2022/buddy-system/ " >Buddy System</a> </li> <li> <a class="" href=" /hackathon2022/team " >Team</a> </li> </ul> <ul id="bottom-navlinks" class="bottom-navlinks visible-xs visible-sm"> </ul> </nav> </header> <div class="content-wrapper"> <div class="jumbotron text-left"> <div class="animated hiding" data-animation="fadeInLeft" data-delay="500"> <h1>HackTrack</h1> </div> </div> </div> </section> <!-- End Top Section --> <!-- About Hackathon Section --> <section id="about-hackathon" class="about-hackathon"> <div class="content-wrapper"> <div class="col-md-8 col-md-offset-2"> <h3>HackTrack projects</h3> <div style="text-align: justify;"> <h6> <br>The <b>HackTrack</b> is the official <b>fun side</b> of a <b>Brainhack</b> event, where people can work together on projects. What projects? Any kind! From <a href="https://github.com/ohbm/hackathon2020/issues/124" target="_blank">exploding brains</a> to <a href="https://github.com/ohbm/hackathon2020/issues/166" target="_blank">resource gathering</a> and <a href="https://github.com/ohbm/hackathon2020/issues/156" target="_blank">data sharing</a>!<br><br> Would you like to propose a project? Just open <a href="https://github.com/ohbm/hackathon2022/issues/new/choose" target="_blank"> an issue on our GitHub repository</a> and fill the template, we will be in touch to help you get going! But be sure to <a href="https://www.humanbrainmapping.org/i4a/pages/index.cfm?pageid=4073" target="_blank"> register</a> first!<br><br> Do you plan to focus on visualization? Are you getting images so weird that they are kind of beautiful? Consider participating in the <em>Beautiful Mistake</em> category of the <a href="https://ohbm-brainart.github.io" target="_blank">BrainArt SIG</a> competition! You can find the submission form <a href="https://docs.google.com/forms/d/e/1FAIpQLSdkfoq-VF_Aw27MD1whBAZCFl6BHldOpOAQ2GWSXFng7fD3Vw/viewform" target="_blank">here</a>.<br><br> </h6> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/jacobsanz97/Pipeline-Discrepancy-Exploration">Effect of Scan Quality on Pipeline Result Discrepancy</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/92">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>As we all know, neuroimaging experiment results are subject to reproducibility issues. Computing the same measure (on the same data) across multiple pipelines can yield different results. We will run FSL, FreeSurfer, and ASHS on a subset of Prevent-AD data to obtain volumetric measures across the three pipelines. We will then find the discrepancies in these measures between the pipelines, and see if we can explain the discrepancies given the properties of the input scans. We will do this by running MRIQC on the scans, and correlating the outputted image quality metrics with the pipelines discrepancies.</p> </p> <h5>SKILLS</h5> <p><p>Python, FreeSurfer, FSL, ASHS</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="img/projects/6e42718cdde26647f3b89a79d74e1a3866d9a93e.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://chrisproject.org">ChRIS Research Integration Service</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/91">github issue</a></h6> <h6> Hubs: Glasgow, Americas </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>ChRIS is a platform for making reproducible, container-based analysis easy to develop and easy to use. Our goal for BrainHack is to get feedback on the application’s user experience, and do development work of wrapping existing software as ChRIS plugins.</p> </p> <h5>SKILLS</h5> <p><p>Optional: docker, python, JSON</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/raamana/confounds">confounds: deconfounding library to properly handle confounds</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/90">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Develop a python library of methods to handle confounds in various neuroscientific analyses, esp. statistics and predictive modeling. More info and slides here: https://crossinvalidation.com/2020/03/04/conquering-confounds-and-covariates-in-machine-learning/</p> </p> <h5>SKILLS</h5> <p><ul> <li>python programming (preferably intermediate or better, but can work with basic skills)</li> <li>some statistics</li> <li>documentation ability</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="img/projects/1b2f98fabd9ca1ca26d5589f06a979079438d727.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/brainlife/brainlife/issues/15">Create local (dev) installation package for brainlife.io</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/88">github issue</a></h6> <h6> Hubs: Europe / Middle East / Africa, Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>brainlife.io is a free and secure reproducible neuroscience analysis platform developed at the University of Texas and Indiana University and collaborators from around the world. It is funded by NSF/NIH. Although brainlife.io is SasS (software as a service) platform, we’d like to provide a capability to easily install brainlife.io on any VM on-prem. The initial goal of this is to allow external developers to more easily run/contribute to the platform development, but I’d like this capability to be used to install brainlife.io instances on other sites. We also need help to improve our documentation site (https://brainlife.io/docs) by identifying content that is confusing / lacking make any necessary content updates.</p> </p> <h5>SKILLS</h5> <p><p>System Administration Docker bash/python scripting (Maybe High Performance Computing)</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="img/projects/cb85c9e79e9852f4450024702ead20aadcab73a4.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/spm/nipype">Creating the SPM M/EEG interface for Nipype</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/87">github issue</a></h6> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://www.fil.ion.ucl.ac.uk/spm/">SPM</a> is an open-source toolbox for analysing fMRI, M/EEG and even PET data, which had its first public release way back in 1991. SPM has been written (primary) in MATLAB, but Python users have been able to exploit a limited set of SPM’s functionality in their Python workflows via <a href="https://nipype.readthedocs.io/en/latest/">Nipype</a>. We seek to widen the scope of what of SPM’s tools can be interfaced with in Nipype (and potentially its successor, <a href="https://pydra.readthedocs.io/en/latest/">Pydra</a>) going forward. For BrainHack, our goal is to create the initial functionality to import, coregister, pre-process and source reconstruct MEG data, a modality currently not supported.</p> </p> <h5>SKILLS</h5> <p><table> <thead> <tr> <th>Skill</th> <th>level of expertise required</th> </tr> </thead> <tbody> <tr> <td>Python</td> <td>beginner</td> </tr> <tr> <td>SPM</td> <td>confirmed</td> </tr> <tr> <td>Git</td> <td>1</td> </tr> </tbody> </table> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/afni/afni/blob/master/src/python_scripts/scripts/gen_group_command.py">Creating a group-level modeling interface in AFNI using Python</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/86">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>There are quite a few powerful modeling tools in AFNI that perform group-level analysis. The programs range from simple to complex ones: GLM, ANCOVA with within-subject (or repeated-measures) factors, linear mixed-effects modeling, nonlinear modeling using smooth splines, intraclass correlation, Bayesian multi-level modeling, etc. These programs include 3dttest++, 3dMVM, 3dLMEr, 3dMSS, 3dISC, 3dICC, RBA, etc. Currently there is a Python program, gen_group_command.py, that provides a nice interface for the user to set up group-level analysis for GLM. Two potential projects are 1) create a unified user interface similar to gen_group_command.py that could generate most, if all, of the group-level programs; alternatively, 2) write a user interface for one or a few group-level programs.</p> </p> <h5>SKILLS</h5> <p><p>Python Shell scripting</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="img/projects/8ea873e2c48cb38d0189582186b3bb7487ef73dd.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/physiopy/physiopy.github.io">Physiopy - Documentation of Physiological Signal Acquisition Best Practices</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/85">github issue</a></h6> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Physiopy is a community formed around developing tools to operate physiological files in MRI setups. This project specifically will expand our package documentation containing tips and strategies on how to collect various forms of physiological data and the use of our packages. We aim to provide a succinct overview of what physiological data are typically recorded during an fMRI experiment, how these signals are recorded, and how these signals can improve our modeling of fMRI time series data. This is an active field of research, and we hope to encourage all users to get the latest recommendations prior to initiating a new study.</p> </p> <h5>SKILLS</h5> <p><p>Excitement for learning the best practices in physiological signal acquisition. We welcome all contributions from any skill set and level, this project will predominantly be focused on documentation contributions. Git - we welcome any level of git (0-3), markdown knowledge helpful</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/ReproNim/ReproTube/">ReproTube: YouTube for Reproducible Neuroimaging</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/84">github issue</a></h6> <h6> Hubs: Glasgow, Americas </h6> <h5>PROJECT DESCRIPTION</h5> <p><ul> <li>establish automated workflow to update/maintain collection of videos, captions, metadata</li> <li>fix up automated captions</li> <li>push fixed up captions back to youtube (research feasibility for automation on that)</li> <li>use/work with DataCat (https://github.com/ohbm/hackathon2022/issues/53) project to provide a convenient front-end</li> <li>establish backup Some rough ideas for more “exciting” projects</li> <li>based on captions/metadata establish “recommendation engine”, express those in metadata, expose via DataCat</li> <li>establish automated extraction/annotation for extra features in videos, e.g.</li> <li>boundaries of sections</li> <li>slides/section titles</li> <li>extract acknowledgement/names from the slides (e.g. grant numbers etc) which otherwise would be missing from metadata/captions</li> <li>establish search engine to search up videos to answer a specific question, e.g.</li> <li>“run fmriprep on HPC”</li> <li>…. provide a query you would like to see …</li> <li><strong>suggest more</strong></li> </ul> </p> <h5>SKILLS</h5> <p><p>We welcome all kinds of contributions from various skills at any level. It could range from proof-reading and fixing up captions, to setting up and writing documentation, discussing relevant functionality, or user-experience-testing, to bash and/or Python-based implementation (or refactoring) of the desired functionality and applying the infrastructure to other use cases, e.g. “MyLabTube” etc.</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/afni/afni/blob/master/src/R_scripts/TRR.R">Implementation of test-retest reliability estimation in Python</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/83">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Accurately estimating reliability is crucial for many fields including neuroimaging and psychometrics. The traditional concept of intraclass correlation is not well-suited for task-related data at the trial level. Thus, a hierarchical modeling approach is desirable to more accurately capture the several levels involved in the data structure. A hierarchical modeling framework has already been implemented into the program TRR using R packages (e.g., brms) in AFNI. It would be a great addition to the neuroimaging and psychometrics communities if the modeling functionality is also implemented using Python. The project is looking for volunteers who are skilled in Python and familiar with some extent of hierarchical modeling. Useful links: https://github.com/adamhaber/pybrms</p> </p> <h5>SKILLS</h5> <p><p>Python R</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/afni/afni/blob/master/src/R_scripts/RBA.R">Implementation of region-based FMRI data analysis through hierarchical modeling using Python</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/82">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Hierarchical modeling is a powerful analytical framework that allows efficient information flow across multiple levels. Compared to the traditional mass univariate analysis, the hierarchical approach has the advantages of spatial specificity, high statistical efficiency, no discrimination against small regions, focus on effect estimation, full result reporting, etc. The modeling capability has already been implemented into the program RBA using R packages (e.g., brms) in AFNI. It would be a great addition to the neuroimaging community if the modeling functionality is also implemented using Python. The project is looking for volunteers who are skilled in Python and familiar with some extent of hierarchical modeling. Useful links: https://github.com/adamhaber/pybrms</p> </p> <h5>SKILLS</h5> <p><p>Python R</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nih-fmrif/code-convergence-summaries/issues/8">Documentation and demo examples for BIDS Datasets: running FitLins with 3dREMLfit in AFNI</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/81">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>FitLins is the numerical engine developed to solve models for FMRI data analysis. 3dREMLfit is an AFNI program that fits subject-level time series data with the temporal correlations of the residuals characterized through an ARMA(1,1) structure. The output includes regression coefficients, their t- and F-statistics (plus the associated degrees of freedom). The functionality of 3dREMLfit has been incorporated into FitLins, but its usage has not been documented. This project solicits volunteers to write documents about model specification using FitLins with 3dREMLfit and to provide demonstrative examples. Below are some related links: https://github.com/nih-fmrif/code-convergence-summaries/issues/8 https://fitlins.readthedocs.io/en/latest/usage.html https://afni.nimh.nih.gov/pub/dist/doc/program_help/3dREMLfit.html</p> </p> <h5>SKILLS</h5> <p><p>BIDS derivatives BIDS Stats Models Subject-level FMRI data analysis AFNI</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="img/projects/8194f67e6d8835c2617fa6784b811c57dce96dfa.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/bids-standard/BEP028_BIDSprov">BIDS Prov</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/80">github issue</a></h6> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Provenance is useful for interpreting and comparing scientific results as well as enabling reusable data and analysis. <a href="https://github.com/bids-standard/BEP028_BIDSprov">BIDS prov</a> is a BIDS extension proposal to formalize a provenance framework for BIDS. The framework proposal is based on “w3c prov” and has been adapted for brain imaging. Currently the proposal is drafted in a google doc and contains most of the specifications needed but is lacking some illustrative examples. In particular, examples with BIDS-derivatives outputs using common neuroimaging software such as <code class="language-plaintext highlighter-rouge">Freesurfer</code> are challenging because not entirely formalized in the BIDS specification. The aim of this project is threefold:</p> <ul> <li>Provide complete examples of the BIDS-derivatives specification for <code class="language-plaintext highlighter-rouge">Freesurfer</code>, <code class="language-plaintext highlighter-rouge">SPM</code>, <code class="language-plaintext highlighter-rouge">FSL</code>, <code class="language-plaintext highlighter-rouge">AFNI</code> or your favorite processing tool</li> <li>Propose updates to the specs as needed</li> <li>Get community feedback on the overall proposal in order to submit a final version</li> </ul> </p> <h5>SKILLS</h5> <p><p>Some experience with BIDS and/or neuroimaging software.</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="img/projects/7db7ee99e4ca80717672120c52dc4e8b4f400a41.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/crnolan/bayesonthebrain">Bayes on the brain</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/79">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>To write a tutorial notebook for Bayesian approaches to analysing fMRI data. We aim to implement the equations from Friston &amp; Penny (2007) Empirical Bayes and hierarchal models (SPM textbook). We also aim to simulate data and implement a basic analysis.</p> </p> <h5>SKILLS</h5> <p><p>skills include going through text resources (SPM textbook), implementing equations/data simulation in python, writing plain language explanations of the equations, going through matlab/SPM functions for implementations, and reading/using the tutorial and providing feedback</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://github.com/Inria-Empenn/narps_open_pipelines/blob/main/static/images/project_illustration.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/Inria-Empenn/narps_open_pipelines">The NARPS open pipelines project</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/78">github issue</a></h6> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://www.nature.com/articles/s41586-020-2314-9"><strong>NARPS</strong></a> is an fMRI many-analyst study in which 70 research teams were asked to analyze the same fMRI dataset with their favorite pipeline. We would like to take advantage of this wide range of pipelines to build a <strong>repository of existing fMRI pipelines</strong> that could be used to investigate the impact of analytical variability. Thus, we aim at reproducing these pipelines (for now, 8 were fully reproduced and validated) and to do so, we have access to a file containing <a href="https://github.com/Inria-Empenn/narps_open_pipelines/blob/main/data/original/analysis_pipelines_for_analysis.xlsx"><strong>textual descriptions of the pipelines</strong></a> as well as the <strong>original code for some of the pipelines</strong>. Our project can be split into <strong>several goals</strong>:</p> <ul> <li><strong>Facilitate the access, the use and the contributions</strong> to the database with good documentation, the development of a standard way of contributing and the use of tools like Docker/Singularity to store data and directly execute the pipeline ;</li> <li><strong>Decoding of the textual descriptions</strong> of the NARPS pipelines to understand what choices were made by researchers at each step of the analysis ;</li> <li><strong>Code adaptation</strong> of each step using NiPype interface with the original software packages to build the workflow of the pipeline ;</li> <li><strong>Computation of results and validation</strong> of the reproduction using original unthresholded and thresholded group level statistical maps published in NeuroVault ;</li> <li>Creation of a <strong>link with NeuroVault</strong> to directly publish the results on the platform.</li> </ul> </p> <h5>SKILLS</h5> <p><p><strong>We welcome all sorts of contributions</strong>. However, knowledge and practice in reproducibility and code sharing practices could help. One or more of the specific skills below would be greatly appreciated:</p> <ul> <li>Theoretical knowledge in fMRI data analysis pipelines</li> <li>Practical skills in fMRI data analysis pipelines with the software packages SPM, FSL and/or AFNI (others also appreciated)</li> <li>BIDS format</li> <li>Python programming language</li> <li>Experience with NiPype interface</li> <li>Docker/Singularity: installation and use of a Docker container, sharing images with Docker hub…</li> <li>Use of Git and Github</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/afni/add-afni-tutorials">Expanding AFNI tutorials/examples</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/77">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>MRI processing can range from simple to complex. Having tutorials and demos helps people learn how to expand their processing repertoire, to make scripts more efficient, or to generate new ideas. AFNI contains a loooot of programs for a wide variety of tasks, across FMRI, DWI/DTI and structural MRI; many programs are even used for processing other modalities such as EEG, MEG, etc. Several tutorials and demos exist in the documentation: https://afni.nimh.nih.gov/pub/dist/doc/htmldoc/tutorials/main_toc.html …. but it would be great to add more. So, this project could create more tutorials/examples/demos, either with Python or shell scripting, and can even involve making a Sphinx-based RST page for adding to the website. Bring your own scripting ideas, and/or find out how to do processing with AFNI.</p> </p> <h5>SKILLS</h5> <p><p>AFNI MRI processing interest/experience Sphinx/RST experience/interest shell scripting and/or Python</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/afni/ap_bids_deriv_names">BIDS derivatives naming for afni_proc.py output</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/76">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>afni_proc.py creates a full pre-processing pipeline for a wide variety of MRI data, including alignment, motion correction, blurring, and regression modeling; outputs can be volumetric or projected onto surfaces, for single- or multi-echo data. Therefore, there is a variety of possible outputs. But afni_proc.py also has a dictionary of specific outputs that it populates, so the outputs are well-characterized and known. It would be nice to formulate rules for BIDS-derivative-appropriate naming for these outputs, for populating a new directory with these NIFTI, JSON and other files.</p> </p> <h5>SKILLS</h5> <p><p>Some useful skills to have would be: BIDS derivatives experience FMRI processing interest/background AFNI Collaborative spirit! perhaps Python, if coding</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://drive.google.com/file/d/1TDsEzr1R2T8O49ryNMqwPVFHjoXDOhaC/view?usp=sharing" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://ramp.studio/problems/stroke_lesions">Machine Learning for Stroke Lesion Segmentation</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/75">github issue</a></h6> <h6> Hubs: Americas </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>We’d love for people interested in machine learning for image segmentation to try out our stroke lesion segmentation challenge! It’s built with the RAMP platform and meant to be a collaborative and educational way to get into new research areas. It has a starter kit for people to get familiar with some basic steps for creating a lesion segmentation algorithm, and learn about packages such as PyBIDS along the way. We just released it and hope people can find it useful to learn and work together on a complicated neuroimaging problem!</p> </p> <h5>SKILLS</h5> <p><p>Familiarity with python/juptyer notebook</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://www.nmind.org/assets/images/brain_wires.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://www.nmind.org/">Review your tool with NMIND</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/73">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://www.nmind.org/">NMIND</a> is a collaborative initiative dedicated to responding to the growing frustration about redundancies in effort and reproducibility in neuroimaging. NMIND seeks to build a community which advances the development of standards in software development, nomenclature, and testing, to ultimately harmonize advancements in neuroscience. The goal of this project is to use the community-designed <a href="https://github.com/nmind/standards-checklist/tree/main/checklists">NMIND software-evaluation checklists</a> to both a) evaluate a tool of interest, and b) feed-back the experience to improve the checklists themselves.</p> </p> <h5>SKILLS</h5> <p><ul> <li>None</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/TomMaullin/BLMM">Big Linear Modelling and Big Linear Mixed Modelling</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/70">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Large-scale, shared datasets are becoming increasingly commonplace in fMRI, challenging existing tools both in terms of overall scale and complexity of the study designs. As sample sizes grow, new opportunities arise to detect and account for grouping factors and covariance structures present in large experimental designs. To facilitate large sample analysis, we have created two Python toolboxes for use on HPC clusters:</p> <ul> <li>“Big” Linear Models (BLM); a toolbox for large-scale distributed fMRI Linear Model analyses.</li> <li>“Big” Linear Mixed Models (BLMM); a toolbox for large-scale distributed fMRI Linear Mixed Model analyses. At present, both tools are functioning and can be used for the analysis of tens of thousands of fMRI images. However, there is plenty that could be improved. Some of the goals we hope to address during the hackathon include: <ol> <li>Developing a rigorous testing suite, potentially with continuous integration via Travis CI.</li> <li>Using Dask to streamline the current code base (at present there are a lot of bash scripts for ‘qsub’bing).</li> <li>Package releases. Neither of the toolboxes are currently on the Python Package Index.</li> <li>Adding customized covariance support. In previous work, we showed how the underlying methods BLMM uses could model custom covariance structures (e.g. AR, Diagonal, Toeplitz etc). However, at present BLMM does not support analyses with these features.</li> </ol> </li> </ul> </p> <h5>SKILLS</h5> <p><ul> <li>Familiarity with the Python programming language (All goals).</li> <li>Familiarity with Travis CI (Goal 1 - recommended but not a necessity).</li> <li>Familiarity with Dask (Goal 2).</li> <li>Understanding of Linear Mixed Models and/or statistics (Goal 4).</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://raw.githubusercontent.com/ofgulban/exploding_brains/master/visuals/example-17.gif" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/ofgulban/exploding_brains">Exploding Brains *in Julia*</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/68">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Particle simulations are used to generate visual effects (in <a href="https://www.youtube.com/watch?v=9H1gRQ6S7gg">movies</a>, <a href="https://www.gdcvault.com/play/1025695/Exploring-the-Tech-and-Design">games</a> etc…). In this project, I want to explore how we can use magnetic resonance imaging (MRI) data to generate interesting visual effects by using (2D) particle simulations. My aim is to convert my previous efforts (see <a href="https://github.com/ohbm/hackathon2020/issues/124">2020 OpenMR Benelux</a>, <a href="https://github.com/OpenMRBenelux/openmrb2020-hackathon/issues/7">2020 OHBM Brainhack</a>) to the <a href="https://julialang.org/">Julia programming language</a>. Why Julia? Because Julia offers convenient parallelization methods. This is very important for speeding up particle simulations, A compilation of my efforts so far can be seen at: https://youtu.be/_5ZDctWv5X4 I am planning to sophisticate the particle simulations and create new animations :)</p> </p> <h5>SKILLS</h5> <p><table> <thead> <tr> <th>language</th> <th>level of expertise required</th> </tr> </thead> <tbody> <tr> <td>Julia (necessary)</td> <td>beginner</td> </tr> <tr> <td>Python (would be useful)</td> <td>confirmed</td> </tr> <tr> <td>C++ (would be useful)</td> <td>confirmed</td> </tr> <tr> <td>Matlab (would be useful)</td> <td>confirmed</td> </tr> </tbody> </table> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://user-images.githubusercontent.com/2774448/173400796-54ac74af-d913-425f-b251-2b8cd7421021.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://neuroscout.org">Neuroscout: A platform for fast and flexible re-analysis of (naturalistic) fMRI studies</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/66">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Neuroscout has two primary goals: 1) to democratize reproducible fMRI analysis by making it trivially easy to specify and fit models to public fMRI datasets and 2) facilitate the analysis of naturalistic datasets by leveraging machine-learning algorithms for automated annotation. Combined, these two goals seek to increase the reproducibility and generalizability of neuroimaging analysis. Neuroscout is already a stable platform. However, we are still working on growing our user base, and expanding the functionality. To that end we propose several goals for this hackathon: 1) Develop and get feedback on a end-to-end tutorial for Neuroscout. Currently, although Neuroscout features sufficient documentation, many users are not yet clear on how to best use the platform. By creating a complete end-to-end tutorial, and receiving feedback from the community, we aim to Neuroscout more accessible. This is a great issue for first time contributors! 2) Add more datasets. Neuroscout currently spans 40 distinct naturalistic stimuli across over a dozen independent datasets. However, many more datasets are made public yearly, and we will seek to further expand the number of datasets indexed. In particular, we hope to also include non-naturalistic datasets, to increase the scope of Neurocout. 3) Add more naturalistic features. Neuroscout uses machine-learning algorithms to annotate naturalistic stimuli such as movies. We developed a library (<em>pliers</em>) to provide a uniform API to diverse algorithms. However, there are many more algorithms that could be incorporated and tested. Many are available to extract but have not been actively worked on. A specific example would be to incorporate DeepGaze, to simulate eye tracking data in datasets without it. Validating new features by building models would be a relatively easy contribution for first timers. 4) Develop multivariate analysis pipelines. Neuroscout currently focuses on fitting multi-stage univariate GLM models using BIDS Stats Models (a formal specification). However, multivariate approaches are widely popular for analyzing naturalistic data. Although we don’t aim to fully specify multivariate models in a standardized format, we aim to prototype Neuroscout-compatible multivariate workflows that can take advantage of the vast number of datasets and features made easily available by the Neuroscout API. In the future, this project could be refined to become a core component of Neuroscout, as an alternative to GLM models.</p> </p> <h5>SKILLS</h5> <p><p>Basic familiarity with neuroimaging analysis. Plus: tutorial writing skills, experience with naturalistic data and multivariate modeling (specifically encoding and decoding models).</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://github.com/neurobagel/browbids/blob/main/public/browbids.png?raw=true" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurobagel/browbids">Browbids - Make the browser run pybids</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/65">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Browsers (typically) don’t run Python code. That’s a shame, because the neuroimaging community has so many great tools written in Python that could be reused in interactive, graphical web apps without requiring users to install anything on their machine. If you’re working with BIDS, you are probably familiar with the <a href="https://bids-standard.github.io/bids-validator/">bids-validator</a> project that you can simply access with your web browser. Wouldn’t it be nice if you could just run some simple pyBIDS queries on your local BIDS dataset like that, directly in the browser, without having to first source your python environment and installing some libraries? And ideally without having to reimplement pyBIDS as a javascript library? Luckily, new solutions for python in the browser are currently being developed: <a href="https://pyodide.org/en/stable/">Pyodide</a> and <a href="https://pyscript.net/">pyscript</a> are two (fairly) recent efforts to bring python to the browser using the <a href="https://webassembly.org/">webassembly</a> language. These projects not only allow us to run Python code in the browser and use the output, but we can also install (some) Python libraries from PyPI and use them directly. So you can install <a href="https://github.com/ANCPLabOldenburg/ancp-bids">ancp_bids</a> inside pyodide, and then load a local BIDS dataset in your browser and run some basic queries. Here is a very simple prototype: <a href="https://browbids.netlify.app/">https://browbids.netlify.app/</a> . But there are still a lot of challenges to solve, particularly with file system access and python dependencies that don’t play nice with pyodide. The rough <strong>goals for this hackathon</strong> are:</p> <ul> <li>understand what the most relevant pyBIDS use cases and queries are that can be implemented in the browser</li> <li>find out how we can make a reusable wrapper / plugin of the pyodide-pyBIDS bundle that other projects can just load to gain this functionality</li> <li>investigate ways to safely access the local filesystem so we can expose file metadata or even content to the python instance</li> <li>document what we have learned for other projects that may be also interested in browserizing a python library</li> <li>show a minimal but useful prototype of parsing a BIDS dataset in the browser using pyBIDS</li> </ul> </p> <h5>SKILLS</h5> <p><p>We are very much starting from the beginning (although there is a simple proof of concept) and are trying to find a good way to address this project. Many of the initial challenges will probably require experience with javascript and web development as well as pyBIDS, but there is also a need for a BIDS user perspective to answer understand what would be useful things to do with pyBIDS in the browser. So any combination of</p> <ul> <li>experience using BIDS (to suggest or discuss use cases)</li> <li>good at writing accessible documentation / tutorials</li> <li>experience with Javascript (JS), any of</li> <li>ideally some accessible frontend framework (e.g. Vue)</li> <li>visualization libraries (e.g. d3)</li> <li>JS testing (e.g. Jest, cypress.io)</li> <li>packaging or distribution (e.g. npm / Vue plugin writing, …)</li> <li>browser filesystems, accessing local filesystem from browser,</li> <li>python</li> <li>familiarity with pybids / acnp_bids python API</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://compose.neurosynth.org/static/synth.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurostuff/neurostore">Neurosynth-Compose User Testing</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/64">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://compose.neurosynth.org">neurosynth-compose</a> is a reimagining of the original neurosynth to give users the power to edit studies and create their own meta-analyses. While it is possible to create and execute a meta-analysis, we do not know what features/fixes people want. Goals:</p> <ul> <li>get people to test the platform and identify pain points</li> <li>identify places where documentation is unclear/missing</li> </ul> </p> <h5>SKILLS</h5> <p><ul> <li>interest in performing a meta-analysis</li> <li>patience to work with our website</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://nimare.readthedocs.io/en/latest/_images/nimare_overview.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurostuff/NiMARE">NiMARE: Neuroimaging Meta-Analytic Research Environment</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/62">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><h4 id="user-guide-project">User Guide Project</h4> <p>Currently, there is a mix of methods descriptions (the <a href="https://nimare.readthedocs.io/en/latest/methods.html">NiMARE Methods</a> page) and <a href="https://nimare.readthedocs.io/en/latest/auto_examples/index.html">examples</a> of NiMARE’s functionality, but those two elements are very separate, and the examples are more a smattering of disconnected exhibitions than a tutorial. We came up with the idea of making the documentation more into a user guide.</p> <h4 id="resources">Resources</h4> <ul class="task-list"> <li>https://nimare.readthedocs.io/en/latest/methods.html</li> <li>https://nimare.readthedocs.io/en/latest/auto_examples/index.html</li> <li>https://github.com/neurostuff/ohbm2021-nimare-tutorial</li> <li>https://github.com/NBCLab/nimare-paper <h4 id="user-guide-goals">User Guide Goals</h4> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Create a jupyter book</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Leverage existing resources (see above)</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Write tutorial documentation with a novice meta-analysis practicitioner in mind</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Have people test jupyter book for typos/understanding/correctness <h4 id="extension-addition-project">Extension addition project</h4> <p>NiMARE covers a wide breadth of meta-analytic algorithms. However, the only constant is change. With new algorithms emerging from research across the globe, there is a need for integrating the latest trends in neuroimaging meta-analysis and making the process smooth for new contributors. In order to kickstart the extension process the goals are twofold:</p> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />integrate a new algorithm into NiMARE</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />document the process and find pain points</li> </ul> </p> <h5>SKILLS</h5> <p><p>Minimally, one of the following:</p> <ul> <li>meta-analysis: beginner</li> <li>python: beginner</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://raw.githubusercontent.com/nipype/pydra/master/docs/logo/pydra_logo.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nipype/pydra">Pydra: Converting existing scientiﬁc workﬂows to the new dataﬂow engine Pydra</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/61">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Pydra is a part of the second generation of the Nipype ecosystem. The goal of this project is to convert existing neuroimaging analytical workﬂows that are written in other languages to the new dataﬂow engine Pydra. The main deliverables of the project are comprehensive Pydra workﬂows that use interfaces from neuroimaging packages such as FSL, SPM, ANTs, AFNI, FreeSurfer. Goals for the Brainhack:</p> <ul> <li>Converting <a href="https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_bids_features.html#sphx-glr-auto-examples-04-glm-first-level-plot-bids-features-py">Nilearn.glm</a> tutorial to Pydra</li> <li>Moving <a href="https://github.com/poldracklab/fitlins">FitLins</a> to Pydra</li> </ul> </p> <h5>SKILLS</h5> <p><p>Minimal:</p> <ul> <li>Python</li> <li>Git Optional:</li> <li>Familiarity with GLM, BIDS</li> <li>Familiarity with Nipype, Nilearn</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://neuosc.com/wp-content/uploads/2022/03/Flux-Logo-1-370x153.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/Neuronal-Oscillations/FLUX">FLUX: A pipeline for MEG analysis and beyond</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/55">github issue</a></h6> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://neuosc.com/flux/">FLUX </a> is a pipeline for analysing magnetoencephalography (MEG) data. By making the analyses steps and setting explicit, it aims to facilitate open science with the larger goal of improving the replicability of MEG research. So far, the FLUX pipeline has been developed for MNE-Python and FieldTrip with a focus on the MEGIN/Elekta system. The goal of this Brainhack project is to make the FLUX pipeline more flexible by making it fully BIDS compatible, as well as expanding its application to other systems, for instance CTF, optically pumped magnetometer (OPM) and electroencephalography (EEG).</p> </p> <h5>SKILLS</h5> <p><p>This is an ongoing project and there are many ways in which you could contribute; from helping to improve the documentation to developing new functionalities, all kinds of contributions are welcome. Any of the following skills will be very helpful:</p> <ul> <li>experience with MEG or any other neurophysiological method</li> <li>basic Python/MATLAB knowledge</li> <li>familiarity with BIDS</li> <li>good writing skills</li> <li>being enthusiastic about Neuroscience!</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://nipy.org/nibabel/_static/reggie.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nipy/nibabel/">Type hints for NiBabel</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/54">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Python has support for <a href="https://docs.python.org/3/library/typing.html">type annotations</a> to help developers code more effectively by catching bugs via static analysis or making auto-complete suggestions. The more libraries that annotate their code with useful type hints, the more effective this assistance becomes. The goal of this project is to annotate NiBabel to ease the development process for neuroimaging in Python and improve the reliability of code built on top of NiBabel. We will use <a href="http://mypy-lang.org/">mypy</a> for static analysis and test out type hinting in VScode.</p> </p> <h5>SKILLS</h5> <p><p>Minimum</p> <ul> <li>Some experience with Python and numpy</li> </ul> <p>Ideal</p> <ul> <li>Familiarity with some NiBabel APIs</li> <li>Experience with the <a href="https://docs.python.org/3/library/typing.html">typing</a> and <a href="https://numpy.org/devdocs/reference/typing.html">numpy.typing</a></li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://raw.githubusercontent.com/jsheunis/ohbm-2022/main/pics/datacat0_hero.svg" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/datalad/datalad-catalog">DataCat: "bring your own data" and auto-generate user-friendly data catalogs</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/53">github issue</a></h6> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><h4 id="summary">Summary</h4> <p>Do you want to learn how to generate a pretty and F.A.I.R. browser-based data catalog from metadata? Do you want to know how you can make your data known to the world, without sharing the actual data content on centralised infrastructure? Do you want to do this for free using open-source tools? YES?! Then “bring” your own data and join our hackathon project!</p> <h4 id="overview">Overview</h4> <p>DataLad Catalog is a free and open source command line tool, with a Python API, that assists with the automatic generation of user-friendly, browser-based data catalogs from structured metadata. It is an extension to <a href="https://www.datalad.org/">DataLad</a>, and together with <a href="https://github.com/datalad/datalad-metalad">DataLad Metalad</a> it brings distributed metadata handling, catalog generation, and maintenance into the hands of users. For a live example of a catalog that was generated using DataLad Catalog, see our <a href="https://datalad.github.io/datalad-catalog/">StudyForrest Demo</a>. The tool is now ready to be tested (and hopefully broken and then fixed!) on a wider range of user data. This is therefore intended to be a “bring your own data” project. If you are interested in metadata handling of (distributed) datasets, and specifically in generating a live catalog from said metadata, join us for a chance to turn your (metadata)data into a pretty browser application!</p> <h4 id="project-goals">Project Goals</h4> <ul> <li>Getting participants up to speed on what DataLad Catalog is and what it can do. This will be done through an initial discussion and by reading the <a href="https://github.com/datalad/tutorials/blob/master/notebooks/catalog_tutorials/datalad_catalog_primer.ipynb">primer</a></li> <li>Giving participants hand-on experience with the catalog generation process, with the use of <a href="https://github.com/datalad/tutorials/blob/master/notebooks/catalog_tutorials">walk-through tutorials</a></li> <li>Creating your own data catalogs</li> <li>Documenting feedback on your experience by creating issues (any and all types of issues are welcome!)</li> <li>Onboarding anyone interested in contributing to this tool in the many ways that are possible</li> </ul> </p> <h5>SKILLS</h5> <p><p>We welcome all kinds of contributions from various skills at any level. From setting up and writing documentation, discussing relevant functionality, or user-experience-testing, to Python-based implementation of the desired functionality and creating real-world use cases and workflows. You can help us with any of the following skills:</p> <ul> <li>You have a dataset (or distributed datasets) for which you’d like to create an online catalog</li> <li>You enjoy breaking user interfaces or pointing out how the interface can be more intuitive</li> <li>You have experience with the Unix command line</li> <li>You are interested in creating accessible documentation</li> <li>You know Python / JavaScript / HTML / VueJS</li> <li>You are interested in learning about the DataLad ecosystem or the process of creating a DataLad extension</li> <li>You are interested in learning about the DataLad metadata handling capabilities and/or the process of creating a DataLad-based metadata extractors</li> <li>You have knowledge of metadata standards in your domain</li> <li>You have knowledge of <a href="https://bids-specification.readthedocs.io/">BIDS</a> and <a href="https://github.com/bids-standard/pybids">pybids</a> (for the specific case of generating BIDS-related metadata, and rendering that in the catalog)</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neuronets/">Nobrainer toolkit and model zoo for deep learning in neuroimaging</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/52">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>The goals of this hackathon are to improve usability of both the nobrainer library and the nobrainer model zoo.</p> </p> <h5>SKILLS</h5> <p><ul> <li>Python</li> </ul> <p>Optional:</p> <ul> <li>Tensorflow</li> <li>Docker/Singularity</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://uvaauas.figshare.com/ndownloader/files/27251792/preview/27251792/preview.jpg" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/piloubazin/AHEAD-brains">Exploring the AHEAD brains together</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/51">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>We recently made available a post-mortem data set including quantitative MRI and microscopy reconstructed in 3D at 200µm (see <a href="https://doi.org/10.1126/sciadv.abj7892">this article</a> for details). The data set is openly accessible on FigShare already, but we would like to do more to help integrate it in other open science platforms to promote collaborative exploration of the data.</p> <p>Goals for the Brainhack:</p> <ul> <li>set up a version of the data set that fits into <a href="https://brainbox.pasteur.fr/">Brainbox</a></li> <li>check how the data is handled by various visualization tools, make recommendations</li> <li>import initial parcellations from automated tools (nighres so far, but others could be run) into the visualizations</li> <li>manually annotate errors, artifacts, inconsistencies</li> <li>delineate new structures collaboratively</li> </ul> </p> <h5>SKILLS</h5> <p><ul> <li>enthusiasm for detail neuroanatomy</li> <li>dealing with data (re)formatting, header manipulation</li> <li>experience with various brain visualization tools</li> <li>ideas to increase collaboration in neuroanatomical atlasing</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://brainhack.org/brainhack_cloud/logo.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://brainhack.org/brainhack_cloud/">Brainhack Cloud</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/50">github issue</a></h6> <h6> Hubs: Asia / Pacific, Glasgow, Europe / Middle East / Africa, Americas </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>A team of brainhack volunteers applied for Oracle Cloud Credits to support open source projects in and around brainhack with powerful cloud resources. This project is about learning about cloud computing and supporting other projects to make best use of the resources :)</p> </p> <h5>SKILLS</h5> <p><p>Interest in cloud computing</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://raw.githubusercontent.com/nilearn/nilearn/main/doc/logos/nilearn-logo.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nilearn/nilearn">Nilearn: Statistics for Neuroimaging in Python</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/48">github issue</a></h6> <h6> Hubs: Europe / Middle East / Africa, Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Nilearn is an open-source Python package for fast and easy analysis and visualization of brain images. It provides statistical and machine-learning tools, with instructive documentation and a friendly community. It includes applications such as multi-voxel pattern analysis (MVPA), decoding, predictive modelling, functional connectivity, and brain parcellations. Moreover, in recent years, Nilearn has expanded to include Generalized Linear Models (GLMs) to analyse functional MRI data. For the Brainhack, we aim to get feedback from the community about some of the highlights of the latest release. These include:</p> <ul> <li>A new module <a href="https://nilearn.github.io/stable/modules/reference.html#module-nilearn.interfaces">nilearn.interfaces</a> to implement loading and saving utilities with various interfaces</li> <li>Ability to provide <a href="https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_hrf.html#sphx-glr-auto-examples-04-glm-first-level-plot-hrf-py">custom hemodynamic response function (HRF)</a> for dealing with non human primate data in GLM analysis</li> <li><a href="https://nilearn.github.io/stable/auto_examples/01_plotting/plot_3d_map_to_surface_projection.html#interactive-plotting-with-plotly">Interactive surface plotting</a> using Plotly engine</li> <li>Improved <a href="https://nilearn.github.io/stable/development.html">contributing documentation</a></li> </ul> <p>Finally, we are always looking to get feedback on our <a href="https://nilearn.github.io/stable/index.html">documentation</a> and to onboard new contributors.</p> </p> <h5>SKILLS</h5> <p><p>We welcome all contributions from various skill sets and levels. This can include opening discussions around improvements to the <a href="https://nilearn.github.io/stable/index.html">documenation</a> and/or <a href="https://github.com/nilearn/nilearn">code base</a>, answering or commenting on questions or <a href="https://github.com/nilearn/nilearn/issues">issues raised on github</a> and <a href="https://neurostars.org/tag/nilearn">neurostars</a>, reviewing <a href="https://github.com/nilearn/nilearn/pulls">pull requests</a>, and <a href="https://nilearn.github.io/stable/development.html#how-to-contribute-to-nilearn">contributing code</a>.</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://user-images.githubusercontent.com/5311102/166160831-a81f55c3-c131-4e12-ab1f-e3f46593c9e5.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/danielemarinazzo/HOI">Higher order informational interactions in neuroimaging</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/47">github issue</a></h6> <h6> Hubs: Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://www.nature.com/articles/s41593-022-01070-0">Higher order interactions are being increasingly used and applied to neuroimaging data</a> To date there isn’t a freely available python toolbox with proper input/output suitable for neuroimaging data. We have a pretty much optimized <a href="https://github.com/danielemarinazzo/HOI">matlab code</a>, and <a href="https://github.com/PranavMahajan25/HOI_toolbox">a functioning, yet not optimized python one</a> by @PranavMahajan25 (with <a href="https://github.com/brainets/hoi_bhk/tree/main/etienne">some improvements</a> by @EtienneCmb). The goal(s) would be:</p> <ul> <li>improve the python implementation, adding statistical tests which are absent at the moment, adapting them from the matlab repository</li> <li>improve speed</li> <li>add input/output from BIDS processed data (MNE/NiLearn)</li> <li>explore solutions for plotting the data (<a href="https://github.com/renzocom/hyperplot">some attempts</a> by @renzocom) <a href="https://discord.gg/VUrQGnF8bc">Discord server for the project</a></li> </ul> </p> <h5>SKILLS</h5> <p><p>Python Matlab would help for the translation, but we can assist Some notions of statistics/probability theory could also help, but not necessary</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://ohbm-environment.org/wp-content/uploads/2021/12/logo_long-1.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nikhil153/fmriprep/blob/carbon-trackers/singularity/carbon_trackers_readme.md">watts_up_compute</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/46">github issue</a></h6> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Integration of compute-tracker tools into neuroimaging pipelines to estimate carbon footprint of image processing. This is an ongoing project by the Sustainability and Environmental Action group (<a href="https://ohbm-environment.org/">SEA-SIG</a>) at the Organisation for Human Brain Mapping (OHBM). In this project we aim at better understanding the environmental costs of commonly used research pipelines and develop tools to help reduce them. Recently there have been several projects that track cpu/gpu “power draws” incurred during a compute task. These statistics can then be translated into carbon-footprint based on your location and time of processing. See <a href="https://docs.google.com/presentation/d/1QdJQ0F1UNCzxqDqa1Wh5E9XIROqnNWOavHcI0jehSpo/edit?usp=sharing">these slides</a> for more info!</p> <p>Available trackers</p> <ol> <li><a href="https://github.com/mlco2/codecarbon">CodeCarbon</a></li> <li><a href="https://github.com/lfwa/carbontracker">CarbonTracker</a></li> <li><a href="https://github.com/Breakend/experiment-impact-tracker">EIT</a></li> </ol> <p>Current implementations</p> <ol> <li>General purpose <a href="https://github.com/neurodatascience/watts_up_compute">wrapper</a> with CodeCarbon and EIT</li> <li>fMRIPrep <a href="https://github.com/nikhil153/fmriprep/blob/carbon-trackers/singularity/carbon_trackers_readme.md">integration</a> with CodeCarbon</li> </ol> <p>Brainhack tasks</p> <ol> <li>Test fMRIPrep integration on multiple hardware</li> <li>Integrate trackers into other neuroimaging pipelines e.g. FSL, SPM etc.</li> </ol> </p> <h5>SKILLS</h5> <p><p>You don’t need to be familiar with all of these, just any subset of these would do!</p> <p>Programming languages</p> <ul> <li>Python</li> <li>Bash</li> <li>Matlab</li> </ul> <p>Neuro-software specific skills</p> <ul> <li>FreeSurfer</li> <li>fMRIPrep</li> <li>NiPype</li> <li>SPM</li> <li>FSL</li> </ul> <p>Data standards</p> <ul> <li>Brain Imaging Data Structure (BIDS)</li> </ul> <p>Git skills</p> <ul> <li>Git - 2: comfortable working with branches and can do a pull request on another repository</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://user-images.githubusercontent.com/29738718/170998973-86081990-33b6-45c2-9d77-21e68aea9053.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/datalad/datalad-dataverse">DataLad-Dataverse integration</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/43">github issue</a></h6> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://dataverse.org">Dataverse</a> is open source research data repository software that is deployed all over the world in data or metadata repositories, so called Dataverse collections. It supports sharing, preserving, citing, exploring, and analyzing research data with descriptive metadata, and thus contributes greatly to open, reproducible, and FAIR science. <a href="https://www.datalad.org">DataLad</a>, on the other hand, is a data management and data publication tool build on <a href="https://git-scm.org">Git</a> and <a href="https://git-annex.branchable.com">git-annex</a>. Its core data structure, DataLad datasets, can version control files of any size, and streamline data sharing, updating, and collaboration. In this hackathon project, we aim to make DataLad interoperable with Dataverse to support dataset transport from and to Dataverse instances. To this end, we will build a new DataLad extension <code class="language-plaintext highlighter-rouge">datalad-dataverse</code>, and would be delighted to welcome <strong>you</strong> onboard of the contributor team.</p> </p> <h5>SKILLS</h5> <p><p>We plan to start from zero with this project, and welcome all kinds of contributions from various skills at any level. From setting up and writing documentation, discussing relevant functionality, or user-experience-testing, to Python-based implementation of the desired functionality and creating real-world use cases and workflows. You can help us with any of the following skills: You have used a Dataverse instance before and/or have access to one, or you are interested in using one in the future - You know technical details about Dataverse, such as its API, or would have fun finding out about them - You know Python - You have experience with the Unix command line - You are interested in creating accessible documentation - You are interested in learning about the DataLad ecosystem or the process of creating a DataLad extension - Your secret hobby is Git plumbing - You know git-annex, and/or about its backends - You want to help create metadata extractors for Dataverse to generate dataset metadata automatically</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://layerfmri.files.wordpress.com/2022/05/image-01.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://layerfmri.com/hackathon22/">MOSAIC for VASO fMRI</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/42">github issue</a></h6> <h6> Hubs: Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Vascular Space Occupancy is an fMRI method that is popular for high-resolution layer-fMRI. Currently, the most popular sequence is the one by Rüdiger Stirnberg from the DZNE in Bonn, which is actively being employed at more than 30 sites. This sequence concomitantly acquires fMRI BOLD and blood volume signals. In the SIEMENS reconstruction pipeline, these signals are mixed together within the same time series, which challenges its user friendliness. Specifically:</p> <ul> <li>The “raw” dicom2nii-converted time-series are not BIDS compatible (see <a href="https://github.com/bids-standard/bids-specification/issues/1001">https://github.com/bids-standard/bids-specification/issues/1001</a>).</li> <li>The order of odd and even BOLD and VASO image TRs is dependent on the nii-converter.</li> <li>Workarounds with 3D distortion correction, results in interpolation artifacts.</li> <li>Workarounds without MOSAIC decorators result in impracticable large data sizes.</li> </ul> <p>The goal of this Hackathon is to extend the 3D-MOSAIC to solve these constraints. This functor is commonly used to sort images by echo-times, by RF-channels, by magnitude and phase in the SIEMENS reconstruction pipeline into sets of mosaics . However currently, this functor does not yet support the dimensionality of SETs. In this project we seek to include SETs into the capabilities of the functor.</p> <p>Acknowledgements:</p> <p>This project is based on previous tests by Rüdiger Stirnberg and Philipp Ehses to isolate the cause of the problem. This project will be based on the mosaic functor that was originally developed by Ben Poser and is currently being further extended by Philipp Ehses. The compatibility of the “raw” data with BIDS are supported by BIDS extensions spear-headed by Remi Gau and supported by Daniel Handwerker. The Hackathon logistics across various internet platforms are kindly guided by our Hackathon mentor Faruk Gulban.</p> </p> <h5>SKILLS</h5> <p><ul> <li>SIEMENS ICE programming in VE</li> <li>C++</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://neurocausal.github.io/images/author/inverted-logo_huf11703388d6cc09ec62cec26261f7e3e_54804_148x148_fit_box_2.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurocausal">NeuroCAUSAL - Development of an Open Source Platform for the Storage, Sharing, Synthesis and Meta-Analysis of Clinical Data</a></h4> <h6><a href="https://github.com/ohbm/hackathon2022/issues/35">github issue</a></h6> <h6> Hubs: Americas, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>We wish to work with clinicians, neuroimagers, and software developers to develop an open source platform for the storage, sharing, synthesis and meta-analysis of human clinical data to the service of the clinical and cognitive neuroscience community so that the future of neuropsychology can be transdiagnostic, open, and FAIR.</p> <p>Following the steps of what enable similar transition in functional neuroimaging, we are breaking down the over-ambitious goal in two stages:</p> <ol> <li>Create a sort of spin-off of <a href="https://neuroquery.org/">Neuroquery</a> that only covers lesion-related data hence allowing causal inferences</li> <li>A <a href="https://neurovault.org/">Neurovault</a> kind of tool facilitating sharing of clinical data, which shall benefit from a sort of “neuropsyhcological BIDS formatting guidelines” (OHBM poster #2066 seems to have read our minds)</li> </ol> </p> <h5>SKILLS</h5> <p><p>We are very heterogeneous in our own skills sets &amp; levels and welcome all sorts of contributions 😄 <br /> The <strong>ontological issues</strong> we are facing require familiarity with <a href="https://github.com/neurocausal/neurocausal/issues/4">neurology</a> and/or <a href="https://github.com/neurocausal/neurocausal/issues/5">cognitive science</a>. These are contentious matters in the field and a perfect solution is not realistic: we seek the good compromise that will make this platform a useful tool for the broad community interested in the future of neuropsychology.<br /> The <strong>technical issues</strong> will benefit from people familiar with tools to scrap data from texts, train/test predictive models, generally speaking converting to code our pipeline of papers selection &gt; model fitting &gt; function-to-structure mapping visualization (<a href="https://github.com/neurocausal/neurocausal_data/issues/1">example</a>).<br /> We are aware we just started scratching the surface and will need lots of help on all fronts 🙏</p> </p> </div> </div> </div> </div> </section> <!-- End About Hackathon Section --> <!-- Begin Footer --> <footer id="footer" class="footer"> <div class="row"> <div class="pull-left col-md-6 col-xs-6"> <div class="g-plusone" data-size="medium" data-annotation="inline" data-width="300" data-href="https://ohbm.github.io/hackathon2022"></div> </div> <div class="logo logo-footer logo-gray pull-right"></div> </div> <div class="row"> <div class="col-md-4 col-xs-6"> <h5>Links</h5> <ul> <li><a href=" https://ossig.netlify.com/ " target="_blank">OHBM Open Science SIG</a></li> <li><a href=" https://www.brainhack.org/ " target="_blank">Brainhack Global</a></li> </ul> </div> <div class="col-md-4 col-xs-6"> <h5>Organizers and contacts</h5> <ul> <li><a href=" https://twitter.com/HaoTingW713 " target="_blank">Hao-Ting Wang (Brainhack Co-chair)</a></li> <li><a href=" https://twitter.com/SteMoia " target="_blank">Stefano Moia (Brainhack Co-chair)</a></li> <li><a href=" mailto:OHBMopenscience@gmail.com " target="_blank">OHBMopenscience@gmail.com</a></li> </ul> </div> <div class="col-md-4 col-xs-6"> <h5>Resources</h5> <ul> <li><a href=" /hackathon2022/coc " target="_blank">Code of Conduct</a></li> <li><a href=" /hackathon2022/contact/ " target="_blank">Contact</a></li> </ul> </div> </div> <div class="row"> <div class="col-md-6 col-xs-12"> <ul class="social-links"> <li> <a href=" https://twitter.com/ohbmopen " target="_blank"> <svg class="icon icon-twitter" viewBox="0 0 30 32"> <use xlink:href="/hackathon2022/img/sprites/sprites.svg#icon-twitter"></use> </svg> </a> </li> <li> <a href=" /hackathon2022/feed.xml " target="_blank"> <svg class="icon icon-rss" viewBox="0 0 30 32"> <use xlink:href="/hackathon2022/img/sprites/sprites.svg#icon-rss"></use> </svg> </a> </li> </ul> </div> </div> <div class="row"> <!-- Please don't delete this line--> <div class="col-md-6"> <p class="copyright"> &copy; 2018 Based on <a href="https://github.com/gdg-x/zeppelin" target="_blank">Project Zeppelin</a>. Designed and created by <a href="https://plus.google.com/+OlehZasadnyy/about" target="_blank">Oleh Zasadnyy</a> &middot; <br>Edited and re-adapted by Elizabeth Levitis and Remi Gau.</a> </p> </div> </div> </footer> <!-- End Footer --> </div> </div> </div> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', '', ''); ga('send', 'pageview'); </script> <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <script> window.jQuery || document.write('<script src="/hackathon2022/js/jquery-2.1.1.min.js><\/script>') </script> <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script> <script> if (typeof($.fn.modal) === 'undefined') { document.write('<script src="/hackathon2022/js/bootstrap.min.js><\/script>') } </script> <script src="/hackathon2022/js/default.js"></script> <script> Waves.displayEffect(); </script> <script src="/hackathon2022/js/scripts.js"></script> <script type="application/ld+json"> [{ "@context" : "http://schema.org", "@type" : "Event", "name" : "OHBM BrainHack 2022", "description": "OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG.", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022", "url" : "https://ohbm.github.io/hackathon2022", "startDate" : "", "doorTime" : "", "endDate" : "", "location" : { "@type" : "Place", "name" : "", "sameAs" : "", "address" : { "@type" : "PostalAddress", "streetAddress" : "", "addressLocality" : "", "addressRegion" : "", "postalCode" : "", "addressCountry" : "" }, "geo" : { "@type" : "GeoCoordinates", "latitude" : "", "longitude" : "" } }, // Not supported yet // "organizer" : { // "@type" : "Organization", // "name" : "OHBM Open Science SIG", // "alternateName" : "", // "description" : "", // "logo" : "https://ohbm.github.io/hackathon2022/hackathon2022", // "email" : "", // "sameAs" : "" // }, "subEvent" : { "@type" : "Event", "name" : "HackTrack", "description": "", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022", "url" : "https://ohbm.github.io/hackathon2022/hackathon2022/hackathon/", "startDate" : "2022-06-16", "doorTime" : "09:00", "endDate" : "2022-06-16", "location" : { "@type" : "Place", "name" : "Queen Margaret Union", "sameAs" : "http://communa.net.ua/", "address" : { "@type" : "PostalAddress", "streetAddress" : "22 University Gardens", "addressLocality" : "Glasgow", "addressRegion" : "", "postalCode" : "G12 8QN", "addressCountry" : "United Kingdom" }, "geo" : { "@type" : "GeoCoordinates", "latitude" : "55.8735962", "longitude" : " -4.2913955" } } }, "offers" : [ { "@type" : "Offer", "name" : "Early Bird", "url" : "http://dfua.ticketforevent.com/", "price" : "350", "priceCurrency" : "UAH", "validFrom" : "2014-08-25T10:00", "validThrough" : "2014-09-30T23:59" } ], "performer" : [ { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" } ], "eventStatus" : "EventScheduled", "typicalAgeRange" : "16+" }] </script> </body> </html>
